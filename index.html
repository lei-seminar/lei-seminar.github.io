<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta content="width=device-width,initial-scale=1" name="viewport"><meta content="#000000" name="theme-color"><meta content="Web site created using create-react-app" name="description"><link href="/favicon.png" rel="shortcut icon"><link href="/manifest.json" rel="manifest"><title>LEI Seminar</title><script defer src="/static/js/main.c2a5efa7.js"></script><link href="/static/css/main.11a21736.css" rel="stylesheet"><style data-styled="active" data-styled-version="5.3.10">.jdpyoQ{background:#0f334a;height:50px;display:flex;padding-left:5em;margin-left:0;-webkit-box-pack:justify;justify-content:space-between}.cNAjdZ{display:flex;-webkit-box-align:center;align-items:center;text-decoration:none;padding:0 1rem;height:100%;cursor:pointer;font-size:20px;font-family:"Bree Serif",serif;color:#fff!important}.cNAjdZ.active{font-weight:700;color:#fff!important}.cNAjdZ:hover{background-color:#2a6082;font-weight:700;text-decoration:none;color:#fff!important}.Whawq{display:none;color:grey}@media screen and (max-width:768px){.Whawq{display:block;position:absolute;top:0;right:0;transform:translate(-100%,75%);font-size:1.8rem;cursor:pointer}}.hqWkvK{display:flex;-webkit-box-align:center;align-items:center;margin-right:-24px}@media screen and (max-width:768px){.hqWkvK{display:none}}</style><link href="https://fonts.googleapis.com" rel="preconnect"><link href="https://fonts.gstatic.com" rel="preconnect"></head><body><noscript>You need to enable JavaScript to run this app.</noscript><div id="root"><nav class="sc-bgqQcB jdpyoQ"><svg class="sc-ewnqHT Whawq" fill="currentColor" height="1em" stroke="currentColor" stroke-width="0" viewBox="0 0 448 512" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"></path></svg><div class="sc-fFGjHI hqWkvK"><h3 style="padding-top:.3em;padding-right:1em;color:#fff">Large Language Models: Reading Group</h3><a href="/presentation" class="sc-gTRrQi cNAjdZ">Presentations</a><a href="/readinglist" class="sc-gTRrQi cNAjdZ">Reading List</a></div></nav><div><div class="[object Object]" style="padding:1em"></div><div class="card-body"><table><tr><th>Date</th><th>Presentation Title</th><th>Speaker</th><th>Affiliation</th><th>Relevant Papers</th></tr><tr><td>10/18/2023</td><td>LLM's Model Distillation</td><td>Kaicheng Wang</td><td>UCSD LEI Research</td><td><ul><li><a href="https://urldefense.com/v3/__https://aclanthology.org/2023.acl-long.754/__;!!Mih3wA!A1cnW49UB7ZYc_dz04A2Yk97BivF5qoLe0P0mYQsR0g-zeuOoce4V3XhI5ya4rR30de8a70DEPb4FQ9atMnlrZHtGw$">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a></li><li><a href="https://urldefense.com/v3/__https://arxiv.org/abs/2305.02301__;!!Mih3wA!A1cnW49UB7ZYc_dz04A2Yk97BivF5qoLe0P0mYQsR0g-zeuOoce4V3XhI5ya4rR30de8a70DEPb4FQ9atMmzka9JNg$">Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes</a></li><li><a href="https://urldefense.com/v3/__https://arxiv.org/abs/2306.11644__;!!Mih3wA!A1cnW49UB7ZYc_dz04A2Yk97BivF5qoLe0P0mYQsR0g-zeuOoce4V3XhI5ya4rR30de8a70DEPb4FQ9atMnkL58Y_g$">Textbooks Are All You Need</a></li><li><a href="Textbooks Are All You Need II">Textbooks Are All You Need II</a></li><li><a href="https://urldefense.com/v3/__https://crfm.stanford.edu/2023/03/13/alpaca.html__;!!Mih3wA!A1cnW49UB7ZYc_dz04A2Yk97BivF5qoLe0P0mYQsR0g-zeuOoce4V3XhI5ya4rR30de8a70DEPb4FQ9atMnboV9SRA$">Alpaca</a></li><li><a href="https://urldefense.com/v3/__https://lmsys.org/blog/2023-03-30-vicuna/__;!!Mih3wA!A1cnW49UB7ZYc_dz04A2Yk97BivF5qoLe0P0mYQsR0g-zeuOoce4V3XhI5ya4rR30de8a70DEPb4FQ9atMkVzzLnqQ$">Vicuna</a></li></ul></td></tr><tr><td>10/11/2023</td><td>LLM's Application in Information Retrieval (IR)</td><td>Xiaoyue Wang</td><td>UCSD LEI Research</td><td><ul><li><a href="https://openreview.net/pdf?id=wCu6T5xFjeJ">BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models</a></li><li><a href="https://arxiv.org/abs/2202.08904">SGPT: GPT Sentence Embeddings for Semantic Search</a></li><li><a href="https://arxiv.org/abs/2304.09542">Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent</a></li><li><a href="https://arxiv.org/abs/2209.11755">Promptagator: Few-shot Dense Retrieval From 8 Examples</a></li><li><a href="https://arxiv.org/abs/2303.15056">ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks</a></li></ul></td></tr><tr><td>10/04/2023</td><td>Chain of Thought</td><td>Evelyn Yee</td><td>UCSD LEI Research</td><td><ul><li><a href="https://arxiv.org/abs/2112.00114">Show Your Work: Scratchpads for Intermediate Computation with Language Models</a></li><li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li></ul></td></tr><tr><td>09/27/2023</td><td>Theory of Contrastive Learning</td><td>Weili Cao</td><td>UCSD LEI Research</td><td><ul><li><a href="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</a></li><li><a href="https://arxiv.org/abs/2104.08821">SimCSE: Simple Contrastive Learning of Sentence Embeddings</a></li><li><a href="https://arxiv.org/abs/2005.10242">Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</a></li><li><a href="https://arxiv.org/abs/1902.09229">A Theoretical Analysis of Contrastive Unsupervised Representation Learning</a></li></ul></td></tr><tr><td>09/19/2023</td><td>LLMs for theorem provers</td><td>Mark Land</td><td>UCSD LEI Research and Elsevier</td><td><ul><li><a href="https://arxiv.org/pdf/2210.12283.pdf">Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs</a></li></ul></td></tr><tr><td>09/12/2023</td><td>Fine-tuning Language Models with Generative Adversarial Feedback</td><td>Liam Powers</td><td>UCSD LEI Research</td><td><ul><li><a href="https://arxiv.org/abs/2305.06176">Fine-tuning Language Models with Generative Adversarial Feedback</a></li></ul></td></tr><tr><td>09/05/2023</td><td>Efficient Transfer Learning (PETL) for Large Transformer Models</td><td>Andre Wang</td><td>UCSD LEI Research</td><td><ul><li><a href="https://arxiv.org/pdf/2305.15348.pdf">READ: Recurrent Adaptation of Large Transformers</a></li><li><a href="https://arxiv.org/abs/2206.06522">LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning</a></li><li><a href="https://arxiv.org/pdf/2106.09685.pdf">LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</a></li></ul></td></tr><tr><td>08/15/2023</td><td>RLHF</td><td>Gil Pasternak</td><td>UCSD LEI Research</td><td><ul><li><a href="https://arxiv.org/pdf/2203.02155.pdf">Training language models to follow instructions with human feedback</a></li></ul></td></tr><tr><td>08/08/2023</td><td>Mixture of Experts</td><td>Prudviraj Naidu</td><td>UCSD LEI Research</td><td><ul><li><a href="https://arxiv.org/abs/1701.06538">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a></li><li><a href="https://arxiv.org/abs/2202.08906">ST-MoE: Designing Stable and Transferable Sparse Expert Models</a></li></ul></td></tr><tr><td>08/01/2023</td><td>Scaling Laws and the Future of LLMs</td><td>Leon Bergen</td><td>UCSD LEI Research</td><td><ul><li><a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a></li><li><a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</a></li></ul></td></tr><tr><td>07/25/2023</td><td>In-Context Learning</td><td>Andre Wang</td><td>UCSD LEI Research</td><td><ul><li><a href="https://arxiv.org/abs/2211.15661v3">What learning algorithm is in-context learning? Investigations with linear models</a></li><li><a href="https://arxiv.org/abs/2111.02080">An Explanation of In-context Learning as Implicit Bayesian Inference</a></li></ul></td></tr><tr><td>07/18/2023</td><td>Language Models are Unsupervised Multitask Learners</td><td>Leon Bergen</td><td>UCSD LEI Research</td><td><ul><li><a href="https://13699837124766677515.googlegroups.com/attach/4e08b5bfbcab/RadfordWuChildLLuanAmodeiSutskever_2018.pdf?part=0.1&view=1&vt=ANaJVrEvETUn2oiAe33Wh2Sja7kOa_AsLlumNE4rm6PHarJQbHNKLzauNyIceMNLbMa_Z2jLM5TphuneCx7bTTyUF5278-ccQqvKzp0N2-gBEopu01clBr0">Language Models are Unsupervised Multitask Learners</a></li></ul></td></tr><tr><td>07/11/2023</td><td>Transformers</td><td>Mohan Paturi</td><td>UCSD LEI Research</td><td><ul><li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li></ul></td></tr></table></div></div></div></body></html>